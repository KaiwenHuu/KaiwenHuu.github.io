{"componentChunkName":"component---src-pages-notes-machinelearning-index-js","path":"/notes/machinelearning/","result":{"data":{"allMarkdownRemark":{"nodes":[{"frontmatter":{"date":"July 23, 2021","title":"Causal Models","slug":"causal-models"},"id":"d84dff76-5903-57db-8dd7-403af551d7a1","excerpt":"Causal Inference"},{"frontmatter":{"date":"July 23, 2021","title":"Reinforcement Learning","slug":"reinforcement-learning"},"id":"ee3a7fcf-fe10-565c-88d3-a15bee17aca6","excerpt":"RL"},{"frontmatter":{"date":"May 2, 2024","title":"Linear Models","slug":"linear-models"},"id":"b3791cce-d28a-5be4-b9d7-49cb8e782e4d","excerpt":"Linear Classfiers Let us assume that we believe that a certain outcome  is a linear combination of  and some unobservable . OLS Estimator We…"},{"frontmatter":{"date":"May 5, 2024","title":"MLE and MAP","slug":"mle-map"},"id":"6835d976-ff0d-5e6d-aa76-d9c81ffb5aa4","excerpt":"Maximum Likelihood Estimation (MLE) Let  where  and each  is iid. Suppose that there is some parameter . Then the joint probability density…"},{"frontmatter":{"date":"May 6, 2024","title":"Neural Network","slug":"neural-network"},"id":"541924f3-33a7-5cda-b697-58b6894d466e","excerpt":"Neural Network Consider  where , and  is some non-linear transformation. The hidden layer is size  and usual candidates for a single hidden…"},{"frontmatter":{"date":"May 7, 2024","title":"Autoencoder","slug":"autoencoder"},"id":"1e30c0c3-e400-5f76-a1d7-52a4d77020ad","excerpt":"Autoencoder Goal: Learn a function  such that . This function is an Autoencoder. Note, we don't want  to be a simple identity matrix here…"},{"frontmatter":{"date":"May 7, 2024","title":"Optimization","slug":"optimization"},"id":"55911636-744f-51ab-b990-c82d1a1cb08d","excerpt":"Gradient Descent Pseudo Code Start with an initial point for . Let this be  Compute the gradient at  to generate a better guess  where  is…"},{"frontmatter":{"date":"May 8, 2024","title":"Convolutional Neural Network","slug":"convolutional-neural-network"},"id":"47292d30-748a-5ef0-8929-c9cb3c0b59f3","excerpt":"Convolutional Neural Network Processing images in a standard deep learning framework typically encounters two major challenges: Utilizing an…"},{"frontmatter":{"date":"May 9, 2024","title":"Recurrent Neural Network","slug":"recurrent-neural-network"},"id":"864087a6-0257-5e4d-add0-f6511ee6f958","excerpt":"Recurrent Neural Network If incoming data  is sequential then it has the structure . Suppose the task is to classify the part of speech (e.g…"},{"frontmatter":{"date":"May 10, 2024","title":"Attention","slug":"attention"},"id":"e280c79a-aa2a-5cfb-9d06-0e3988643120","excerpt":"Attention Instead of analyzing data sequentially as in an RNN or its variants, most modern approaches analyze sequential information as a…"},{"frontmatter":{"date":"May 11, 2024","title":"Bayesian Inference","slug":"bayeseian-inference"},"id":"ba6de451-cc11-5965-887f-26ce52132f66","excerpt":"In a frequentist setting, a statistician or econometrician typically commits to one specific parameter  after observing  and  and uses this…"},{"frontmatter":{"date":"May 12, 2024","title":"Variational Autoencoder","slug":"variational-autoencoder"},"id":"b19d94df-acdc-5d4d-bbdb-f0df25f66dd1","excerpt":"Variational Autoencoder (VAE) KL Divergence ELBO"}]}},"pageContext":{}},"staticQueryHashes":[],"slicesMap":{}}